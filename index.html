<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="EngageSense: Real-time audience engagement analysis">
    <title>EngageSense</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        /* General Styles */
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #1a1a1a;
            color: #f5f5f5;
        }

        header {
            background-color: #20232a;
            padding: 20px;
            text-align: center;
        }

        header h1 {
            margin: 0;
            font-size: 2rem;
            color: #61dafb;
        }

        main {
            padding: 20px;
        }

        .container {
            max-width: 800px;
            margin: auto;
        }

        .chart-container {
            margin: 20px 0;
        }

        footer {
            background-color: #20232a;
            padding: 10px;
            text-align: center;
            color: #f5f5f5;
        }

        button {
            padding: 10px 20px;
            margin: 10px 5px;
            background-color: #61dafb;
            border: none;
            border-radius: 5px;
            color: black;
            cursor: pointer;
        }

        button:hover {
            background-color: #21a1f1;
        }

        input[type="file"] {
            margin: 20px 0;
            padding: 10px;
        }
    </style>
</head>
<body>
    <header>
        <h1>EngageSense</h1>
        <p>Analyze real-time audience engagement with AI.</p>
    </header>
    <main>
        <div class="container">
            <h2>Upload Audio for Analysis</h2>
            <input type="file" id="audio-upload" accept=".mp3,.wav">
            <button onclick="analyzeAudio()">Analyze Audio</button>
            <button onclick="startRecording()">Start Recording</button>
            <button onclick="stopRecording()">Stop Recording</button>

            <h3>Engagement Levels</h3>
            <div class="chart-container">
                <canvas id="engagementChart"></canvas>
            </div>
            <h3>Feedback</h3>
            <div id="feedback"></div>
        </div>
    </main>
    <footer>
        <p>&copy; 2024 EngageSense. All rights reserved.</p>
    </footer>

    <script>
        let audioContext;
        let analyser;
        let dataArray;
        let mediaRecorder;
        let recordedChunks = [];
        let model;

        // Load the TensorFlow.js model
        async function loadModel() {
            model = await tf.loadLayersModel('path/to/model.json'); // Replace with actual model path
            console.log('Model loaded successfully!');
        }

        // Real-time Visualization
        function visualizeAudio(stream) {
            const canvas = document.getElementById('engagementChart');
            const ctx = canvas.getContext('2d');
            audioContext = new AudioContext();
            analyser = audioContext.createAnalyser();

            const source = audioContext.createMediaStreamSource(stream);
            source.connect(analyser);
            analyser.fftSize = 2048;

            dataArray = new Uint8Array(analyser.frequencyBinCount);

            function draw() {
                analyser.getByteFrequencyData(dataArray);

                ctx.clearRect(0, 0, canvas.width, canvas.height);
                ctx.fillStyle = '#1a1a1a';
                ctx.fillRect(0, 0, canvas.width, canvas.height);

                const barWidth = (canvas.width / dataArray.length) * 2.5;
                let barHeight;
                let x = 0;

                for (let i = 0; i < dataArray.length; i++) {
                    barHeight = dataArray[i];
                    ctx.fillStyle = 'rgb(' + (barHeight + 100) + ',50,50)';
                    ctx.fillRect(x, canvas.height - barHeight / 2, barWidth, barHeight / 2);
                    x += barWidth + 1;
                }

                requestAnimationFrame(draw);
            }

            draw();
        }

        // Start Recording
        function startRecording() {
            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    mediaRecorder = new MediaRecorder(stream);
                    mediaRecorder.start();
                    mediaRecorder.ondataavailable = e => recordedChunks.push(e.data);

                    visualizeAudio(stream);
                })
                .catch(err => console.error('Error accessing microphone:', err));
        }

        // Stop Recording
        function stopRecording() {
            mediaRecorder.stop();
            mediaRecorder.onstop = () => {
                const blob = new Blob(recordedChunks, { type: 'audio/wav' });
                const fileReader = new FileReader();
                fileReader.onload = () => {
                    const audioBuffer = fileReader.result;
                    analyzeAudio(audioBuffer);
                };
                fileReader.readAsArrayBuffer(blob);
            };
        }

        // Analyze Audio
        async function analyzeAudio(audioBuffer) {
            if (!model) {
                alert('Model not loaded. Please wait.');
                return;
            }

            const tensor = tf.tensor(audioBuffer); // Preprocess the audioBuffer here
            const predictions = model.predict(tensor).dataSync();

            const feedback = document.getElementById('feedback');
            feedback.innerHTML = `
                Physical: ${predictions[0].toFixed(2)}%<br>
                Emotional: ${predictions[1].toFixed(2)}%<br>
                Mental: ${predictions[2].toFixed(2)}%<br>
                Spiritual: ${predictions[3].toFixed(2)}%
            `;

            updateChart(predictions);
        }

        // Update Chart with Predictions
        function updateChart(data) {
            const ctx = document.getElementById('engagementChart').getContext('2d');
            new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['Physical', 'Emotional', 'Mental', 'Spiritual'],
                    datasets: [{
                        label: 'Engagement Levels (%)',
                        data,
                        backgroundColor: ['#ff6384', '#36a2eb', '#cc65fe', '#ffce56'],
                    }]
                },
                options: {
                    scales: {
                        y: { beginAtZero: true }
                    }
                }
            });
        }

        // Load the ML model on startup
        loadModel();
    </script>
</body>
</html>
